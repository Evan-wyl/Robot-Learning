# Robot Learning
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![MIT License](https://img.shields.io/badge/license-MIT-green.svg)](https://opensource.org/licenses/MIT) [![LICENSE](https://img.shields.io/badge/license-Anti%20996-blue.svg)](https://github.com/996icu/996.ICU/blob/master/LICENSE)

Papers, codes, datasets, tasks, applications, tutorials.

**Widely used by top conferences and journals:**

- Conferences: [[RSS](https://roboticsconference.org/)] [[IROS](https://ieee-iros.org/)] [[ICRA](https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra)] [[CoRL](https://www.corl.org/)] [[NeurlPS](https://nips.cc/)] [[ICLR](https://iclr.cc/)] [[PMLR](https://proceedings.mlr.press/)] [[ICML](https://icml.cc/)]
- Journals: [[Science Robotics](https://www.science.org/journal/scirobotics)]



## 0.Survey(综述)

#### Vision-Language-Action Model

[2021] [On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)

[2023] [Foundation Models in Robotics: Applications, Challenges, and the Future](https://arxiv.org/abs/2312.07843)

[2023] [Robot Learning in the Era of Foundation Models: A Survey](https://arxiv.org/abs/2311.14379)

[2023] [Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis](https://arxiv.org/abs/2312.08782)

#### Safe Reinforcement Learning

[2023] [State-wise Safe Reinforcement Learning: A Survey](https://arxiv.org/abs/2302.03122)

#### Visions

[2024] [3D Gaussian as a New Vision Era: A Survey](https://arxiv.org/abs/2402.07181)



## 1.Robot Learning Areas and Papers(研究领域与论文)

- [Vision Language Action](https://github.com/whaleRobot/Robot-Learning/blob/master/codes/VLA.md)
- [Locomotion](https://github.com/whaleRobot/Robot-Learning/blob/master/papers/locomotion.md)
- [Safe Reinforcement Learning](https://github.com/whaleRobot/Robot-Learning/blob/master/papers/safe-rl.md)
- [Vision Reinforcement Learning](https://github.com/whaleRobot/Robot-Learning/blob/master/papers/vision-rl.md)



## 3.Code(代码)

- [Vision Lanuage Model](https://github.com/whaleRobot/Robot-Learning/tree/master/codes/VLM)
- [Vision Lanuage Action](https://github.com/whaleRobot/Robot-Learning/blob/master/codes/VLA.md)
- [Locomotion](https://github.com/whaleRobot/Robot-Learning/tree/master/codes/locomotion)
- [Manipulation](https://github.com/whaleRobot/Robot-Learning/tree/master/codes/manipulation)
- [Bimanual Manipulation](https://github.com/whaleRobot/Robot-Learning/blob/master/codes/bimanual.md)
- [Planning](https://github.com/whaleRobot/Robot-Learning/blob/master/codes/planning.md)



## 4.Datasets and Benchmarks(数据集与评测结果)

Please see [HERE](https://github.com/whaleRobot/Robot-Learning/tree/master/data) for the popular robot learning **datasets, simulator and benchmark** results.

[这里](https://github.com/whaleRobot/Robot-Learning/tree/master/data)整理了常用的公开数据集、仿真环境和一些已发表的文章在这些数据集上的实验结果。



## 5.Tasks(任务)

- [Manipulation](https://github.com/whaleRobot/Robot-Learning/blob/master/tasks/manipulation.md)



## Contributing (欢迎参与贡献)

If you are interested in contributing, please refer to [HERE](https://github.com/Evan-wyl/Robot-Learning/blob/master/CONTRIBUTING.md) for instructions in contribution.

------

### Copyright notice

> ***[Notes]This Github repo can be used by following the corresponding licenses. I want to emphasis that it may contain some PDFs or thesis, which were downloaded by me and can only be used for academic purposes. The copyrights of these materials are owned by corresponding publishers or organizations. All this are for better adademic research. If any of the authors or publishers have concerns, please contact me to delete or replace them.***
