## Fine-Tuning

### Survey

[2023] [Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment](https://arxiv.org/abs/2312.12148)

[2024] [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608)



### Large Language Model

[2022] [On the Effectiveness of Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2211.15583)

[2022] [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)

[2023] [Instruction Tuning with GPT-4](https://arxiv.org/abs/2304.03277)

[2023] [Symbol tuning improves in-context learning in language models](https://arxiv.org/abs/2305.08298)

[2023] [Parameter-efficient fine-tuning of large-scale pre-trained language models](https://www.nature.com/articles/s42256-023-00626-4.pdf)

[2024] [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](https://arxiv.org/abs/2404.02948)

#### LoRA

[2021] [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

[2023] [Punica: Multi-Tenant LoRA Serving](https://arxiv.org/abs/2310.18547)

[2023] [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

[2024] [LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report](https://arxiv.org/abs/2405.00732)

[2024] [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)

[2024] [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354)
