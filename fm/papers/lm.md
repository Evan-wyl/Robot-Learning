## Large Language Model

### Survey

[2023] [The Rise and Potential of Large Language Model Based Agents: A Survey](https://arxiv.org/abs/2309.07864)

[2023] [Efficient Large Language Models: A Survey](https://arxiv.org/abs/2312.03863)

[2024] [KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents](https://arxiv.org/abs/2403.03101)

[2024] [Efficient Prompting Methods for Large Language Models: A Survey](https://arxiv.org/abs/2404.01077)

[2024] [On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models](https://arxiv.org/abs/2403.04204)

[2024] [A Survey on Large Language Model based Autonomous Agents](https://arxiv.org/abs/2308.11432)



### Papers

[2023] [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)

[2024] [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905)

[2024] [Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744)

[2024] [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)



## Large Vision Model

[2021] [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)

[2024] [Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction](https://arxiv.org/abs/2404.02905)



## Multi-Modal Large Model

### Survey

| Papers                                                       | Recommendation Index                 |
| ------------------------------------------------------------ | ------------------------------------ |
| [2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) |                                      |
| [2023] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2311.13165) |                                      |
| [2023] [Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey](https://arxiv.org/abs/2312.16602) | :star::star::star::star::star:       |
| [2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) |                                      |
| [2024] [How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model](https://arxiv.org/abs/2311.07594) |                                      |
| [2024] [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451) |                                      |
| [2024] [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) | :star::star::star::star::star::star: |
| [2024] [Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://arxiv.org/abs/2404.07214) |                                      |



### Papers

[2024] [VisionLLaMA: A Unified LLaMA Interface for Vision Tasks](https://arxiv.org/abs/2403.00522)

[2024] [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865)

[2024] [SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/abs/2404.14396)

