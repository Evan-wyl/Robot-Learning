## Multi-Modal Large Model

### Survey

| Papers                                                       | Recommendation Index                 |
| ------------------------------------------------------------ | ------------------------------------ |
| [2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) |                                      |
| [2023] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2311.13165) |                                      |
| [2023] [Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey](https://arxiv.org/abs/2312.16602) | :star::star::star::star::star:       |
| [2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549) |                                      |
| [2024] [How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model](https://arxiv.org/abs/2311.07594) |                                      |
| [2024] [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451) |                                      |
| [2024] [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) | :star::star::star::star::star::star: |
| [2024] [Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://arxiv.org/abs/2404.07214) |                                      |



### Vision Language Model

[2024] [VisionLLaMA: A Unified LLaMA Interface for Vision Tasks](https://arxiv.org/abs/2403.00522)

[2024] [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865)

[2024] [VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing](https://vitron-llm.github.io/)

[2024] [Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference](https://arxiv.org/abs/2403.14520)

[2024] [SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/abs/2404.14396)



### Audio Vision Language Model

[2021] [AudioCLIP: Extending CLIP to Image, Text and Audio](https://arxiv.org/abs/2106.13043v1)

[2022] [CLAP: Learning Audio Concepts From Natural Language Supervision](https://arxiv.org/abs/2206.04769)

[2023] [MusicLM: Generating Music From Text](https://arxiv.org/abs/2301.11325)

[2023] [Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2301.02111)



### Audio Language Model

[2022] [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)



### 3D Representation Vision Language Model

[2022] [ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding](https://arxiv.org/abs/2212.05171)

[2022] [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751)

[2023] [CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition](https://arxiv.org/abs/2303.11313)

[2023] [OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation](https://arxiv.org/abs/2301.07525)

[2023] [3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/abs/2307.12981)

[2023] [OpenScene: 3D Scene Understanding with Open Vocabularies](https://arxiv.org/abs/2211.15654)

[2023] [LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning](https://arxiv.org/abs/2311.18651)

[2024] [SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities](https://arxiv.org/abs/2401.12168)

