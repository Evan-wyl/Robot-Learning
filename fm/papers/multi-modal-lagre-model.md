***If you would like to deeply understand for Multi-Modal Large Model in Robotics, Please read [Real-World Robot Applications of Foundation Models: A Review](https://arxiv.org/abs/2402.05741)***

## Multi-Modal Large Model

### Survey

[2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)

[2023] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2311.13165)

[2023] [Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey](https://arxiv.org/abs/2312.16602)

[2023] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)

[2024] [How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model](https://arxiv.org/abs/2311.07594)

[2024] [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451)

[2024] [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601)

[2024] [Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://arxiv.org/abs/2404.07214)

[2024] [Multimodal Fusion on Low-quality Data: A Comprehensive Survey](https://arxiv.org/abs/2404.18947)



### Vision Language Model

[2024] [VisionLLaMA: A Unified LLaMA Interface for Vision Tasks](https://arxiv.org/abs/2403.00522)

[2024] [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865)

[2024] [VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing](https://vitron-llm.github.io/)

[2024] [Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference](https://arxiv.org/abs/2403.14520)

[2024] [SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/abs/2404.14396)



### Audio Vision Language Model

[2021] [AudioCLIP: Extending CLIP to Image, Text and Audio](https://arxiv.org/abs/2106.13043v1)

[2022] [CLAP: Learning Audio Concepts From Natural Language Supervision](https://arxiv.org/abs/2206.04769)

[2023] [MusicLM: Generating Music From Text](https://arxiv.org/abs/2301.11325)

[2023] [Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers](https://arxiv.org/abs/2301.02111)



### Audio Language Model

[2022] [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)



### 3D Representation Vision Language Model

[2022] [ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding](https://arxiv.org/abs/2212.05171)

[2022] [Point-E: A System for Generating 3D Point Clouds from Complex Prompts](https://arxiv.org/abs/2212.08751)

[2023] [CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition](https://arxiv.org/abs/2303.11313)

[2023] [OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation](https://arxiv.org/abs/2301.07525)

[2023] [3D-LLM: Injecting the 3D World into Large Language Models](https://arxiv.org/abs/2307.12981)

[2023] [OpenScene: 3D Scene Understanding with Open Vocabularies](https://arxiv.org/abs/2211.15654)

[2023] [LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning](https://arxiv.org/abs/2311.18651)

[2024] [SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities](https://arxiv.org/abs/2401.12168)



### More than One Modal

[2022] [Human Motion Diffusion Model](https://arxiv.org/abs/2209.14916)

[2023] [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665)

[2023] [Meta-Transformer: A Unified Framework for Multimodal Learning](https://arxiv.org/abs/2307.10802)

[2023] [FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects](https://arxiv.org/abs/2312.08344)

[2023] [T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations](https://arxiv.org/abs/2301.06052)

[2023] [GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents](https://arxiv.org/abs/2303.14613)
