## Vision Language Action

[2022] [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)

[2023] [Vision-Language Foundation Models as Effective Robot Imitators](https://arxiv.org/abs/2311.01378)

[2023] [Compositional Foundation Models for Hierarchical Planning](https://arxiv.org/abs/2309.08587)

[2023] [Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models](https://arxiv.org/abs/2310.10639)

[2023] [SuSIE: Subgoal Synthesis via Image Editing](https://rail-berkeley.github.io/susie/)

[2024] [3D-VLA: A 3D Vision-Language-Action Generative World Model](https://arxiv.org/abs/2403.09631)

[2024] [SUGAR: Pre-training 3D Visual Representations for Robotics](https://arxiv.org/abs/2404.01491)

[2024] [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)