## Representation Learning for Manipulation

[2022] [R3M: A Universal Visual Representation for Robot Manipulation](https://arxiv.org/abs/2203.12601)

[2023] [Language-Driven Representation Learning for Robotics](https://arxiv.org/abs/2302.12766)

[2023] [Affordances from Human Videos as a Versatile Representation for Robotics](https://arxiv.org/abs/2304.08488)

[2023] [AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains](https://arxiv.org/abs/2212.08333)

[2024] [Recasting Generic Pretrained Vision Transformers As Object-Centric Scene Encoders For Manipulation Policies](https://arxiv.org/abs/2405.15916)

[2024] [Learning Manipulation by Predicting Interaction](https://arxiv.org/abs/2406.00439)

[2024] [QueST: Self-Supervised Skill Abstractions for Learning Continuous Control](https://arxiv.org/abs/2407.15840)



### Relational Keypoint Constraints Representation

[2024] [ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation](https://rekep-robot.github.io/)



### Dense Object Descriptor

[2018] [Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation](https://arxiv.org/abs/1806.08756)

[2021] [Skeleton Merger: an Unsupervised Aligned Keypoint Detector](https://arxiv.org/abs/2103.10814)

[2024] [HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through Contrastive Learning](https://arxiv.org/abs/2408.08312)



### Dense Correspondence Learning

[2019] [Unsupervised Learning of Dense Shape Correspondence](https://openaccess.thecvf.com/content_CVPR_2019/papers/Halimi_Unsupervised_Learning_of_Dense_Shape_Correspondence_CVPR_2019_paper.pdf)

[2021] [Efficient deformable shape correspondence via multiscale spectral manifold wavelets preservation](https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Efficient_Deformable_Shape_Correspondence_via_Multiscale_Spectral_Manifold_Wavelets_Preservation_CVPR_2021_paper.pdf)

[2023] [Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation](https://arxiv.org/abs/2303.11057)

[2024] [EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning](https://arxiv.org/abs/2408.01953)



### Affordance

[2021] [Where2Act: From Pixels to Actions for Articulated 3D Objects](https://arxiv.org/abs/2101.02692)

[2023] [Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects](https://arxiv.org/abs/2309.07473)

[2023] [Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation](https://arxiv.org/abs/2303.11057)

[2024] [Learning Precise Affordances from Egocentric Videos for Robotic Manipulation](https://arxiv.org/abs/2408.10123)